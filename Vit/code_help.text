Mymodel.py

VisionTransformerクラス

init()




forward()
torch.Tensor.view()とtorch.Tensor.reshapeの使い分け
"""
x = x.view(bs, c, h//self.patch_size, self.patch_size, \
            w//self.patch_size, self.patch_size)

"""
ここでは view() を使用。view() は連続したメモリを持つテンソルでのみ動作。

この場合、入力テンソル x は連続したメモリ（連続性がある）を持っていることが前提になっています。以下の理由で view() が適しています：

入力テンソル x は、[32, 3, 32, 32] という形状で最初に生成され、テンソルをそのまま再構成しているだけ。
この時点ではメモリの非連続性を引き起こす操作（例: 軸の並べ替えやスライス操作）が行われていないため、テンソルは連続性を維持している。
そのため、view() を使うことで、メモリ再配置のオーバーヘッドを避けつつ効率的に形状を変更できる。

"""
x.permute(0, 2, 4, 1, 3, 5) 
x = x.reshape(bs, (h//self.patch_size)*(w//self.patch_size), -1)
"""
この部分では、reshape() を使用している。reshape() は、連続したメモリを持っていない場合でも安全に動作するので適している：

その直前の操作（permute）で、テンソルの軸の順序が変更されている。この操作によってテンソルが非連続なメモリ構造になっている可能性が高い。
非連続なテンソルに対して view() を使うとエラーが発生するため、reshape() を使用する必要はある

VisionTransformerクラス


forward()
nn.Linear()の出力に対する.view()の適用
"""
qkv = self.expantion_layer(x) # 全結合層
qkv = qkv.view(bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)       
"""
nn.Linear の出力に torch.view を使える理由
nn.Linear の出力は通常、計算の過程で新たに生成されたテンソルであり、デフォルトでは連続的なメモリレイアウトを持ってる
このため、nn.Linear の出力には直接 torch.view を適用しても問題ない

"""
qlv.unbind(0)
"""
torch.unbind() は PyTorch の関数で、指定した次元に沿ってテンソルを分解し、タプルとして返す

例
"""
import torch

# サンプルテンソル
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

# 第0次元 (行) に沿って分解
rows = torch.unbind(x, dim=0)
print(rows)
# 出力: (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))
# 第1次元 (列) に沿って分解
cols = torch.unbind(x, dim=1)
print(cols)
# 出力: (tensor([1, 4, 7]), tensor([2, 5, 8]), tensor([3, 6, 9]))
"""